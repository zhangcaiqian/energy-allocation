import OpenAI from "openai";

const client = new OpenAI({
  apiKey: process.env.DASHSCOPE_API_KEY,
  baseURL: "https://dashscope.aliyuncs.com/compatible-mode/v1",
});

const LLM_MODEL = process.env.DASHSCOPE_MODEL || "qwen-plus";

const COACH_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä¸ªæ¸©æš–çš„ç²¾åŠ›æ•™ç»ƒï¼Œåå«ã€ŒçŸ¥ç§‹ã€ï¼Œæ˜¯ã€Œç•™ç™½ã€App çš„ AI ä¼™ä¼´ã€‚

ä½ çš„é£æ ¼ï¼š
- æ¸©æš–ã€ç®€çŸ­ã€ä¸è¯´æ•™ã€åƒæœ‹å‹ä¸€æ ·å…³å¿ƒå¯¹æ–¹
- å›å¤æ§åˆ¶åœ¨ 150 å­—ä»¥å†…
- å¦‚æœç”¨æˆ·ç²¾åŠ›å……æ²›ï¼Œç»™äºˆè‚¯å®šå’Œé¼“åŠ±
- å¦‚æœç”¨æˆ·ç²¾åŠ›ä½ï¼Œè¡¨è¾¾ç†è§£å’Œå…³æ€€ï¼Œç»™ä¸€ä¸ªå…·ä½“å°å»ºè®®
- ä¸è¦ç”¨æ„Ÿå¹å·è½°ç‚¸ï¼Œä¸è¦è¯´"åŠ æ²¹"
- å¯ä»¥å¶å°”ç”¨ä¸€ä¸ª emojiï¼Œä½†ä¸è¦è¿‡å¤š
- ç”¨"ä½ "è€Œé"æ‚¨"
- å¶å°”ç”¨è‡ªç„¶ç•Œçš„æ¤ç‰©ã€åŠ¨ç‰©çš„æ¯”å–»ï¼Œç›®çš„æ˜¯è®©ç”¨æˆ·çŸ¥é“é€‚æ—¶ä¼‘æ¯æ‰æ˜¯è‡ªç„¶ç•Œçš„æ³•åˆ™

æ ¸å¿ƒç†å¿µï¼šç•™ç™½ = æœ‰æ„è¯†åœ°ä¿ç•™ç²¾åŠ›ç©ºé—´ç”¨äºæ¢å¤ï¼Œä¸é€æ”¯ã€‚`;

const levelLabels: Record<string, string> = {
  HIGH: "ç²¾åŠ›å……æ²› ğŸŸ¢",
  MEDIUM: "çŠ¶æ€å°šå¯ ğŸŸ¡",
  LOW: "æœ‰ç‚¹ç–²æƒ« ğŸŸ ",
  EXHAUSTED: "å¿«æ²¡ç”µäº† ğŸ”´",
};

function buildCheckInContext(
  level: string,
  question: string,
  todayCheckIns: { level: string; checkInAt: string }[],
  recentSummaries: { date: string; avgScore: number; minScore: number }[]
): string {
  return `## å½“å‰ check-in
- é—®é¢˜ï¼š${question}
- ç”¨æˆ·é€‰æ‹©ï¼š${levelLabels[level] || level}

## ä»Šå¤©çš„è®°å½•
${
  todayCheckIns.length > 0
    ? todayCheckIns
        .map(
          (c) =>
            `- ${c.checkInAt.split("T")[1]?.slice(0, 5)}: ${levelLabels[c.level] || c.level}`
        )
        .join("\n")
    : "ä»Šå¤©çš„ç¬¬ä¸€æ¬¡è®°å½•"
}

## è¿‘å‡ å¤©è¶‹åŠ¿
${
  recentSummaries.length > 0
    ? recentSummaries
        .slice(0, 3)
        .map(
          (s) =>
            `- ${s.date}: å‡å€¼ ${s.avgScore.toFixed(2)}, æœ€ä½ ${s.minScore.toFixed(2)}`
        )
        .join("\n")
    : "æš‚æ— å†å²æ•°æ®ï¼ˆæ–°ç”¨æˆ·ï¼‰"
}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç»™å‡ºæ¸©æš–ç®€çŸ­çš„å›åº”ã€‚`;
}

// ============================================================
// æµå¼ç”Ÿæˆ Check-in å›åº” (è¿”å› ReadableStream)
// ============================================================
export function streamCheckInResponse(
  level: string,
  question: string,
  todayCheckIns: { level: string; checkInAt: string }[],
  recentSummaries: { date: string; avgScore: number; minScore: number }[]
): ReadableStream<Uint8Array> {
  const context = buildCheckInContext(level, question, todayCheckIns, recentSummaries);
  const encoder = new TextEncoder();

  return new ReadableStream({
    async start(controller) {
      const startTime = Date.now();
      const requestId = Math.random().toString(36).slice(2, 8);

      console.log(`[LLM][${requestId}] â–¶ æ¨¡å‹è°ƒç”¨å¼€å§‹`);
      console.log(`[LLM][${requestId}]   æ¨¡å‹: ${LLM_MODEL}`);
      console.log(`[LLM][${requestId}]   ç²¾åŠ›ç­‰çº§: ${level}`);
      console.log(`[LLM][${requestId}]   ä»Šæ—¥å·²è®°å½•: ${todayCheckIns.length} æ¬¡`);
      console.log(`[LLM][${requestId}]   å†å²æ‘˜è¦: ${recentSummaries.length} å¤©`);

      try {
        const stream = await client.chat.completions.create({
          model: LLM_MODEL,
          messages: [
            { role: "system", content: COACH_SYSTEM_PROMPT },
            { role: "user", content: context },
          ],
          max_tokens: 200,
          temperature: 0.8,
          stream: true,
        });

        let fullContent = "";
        let tokenCount = 0;

        for await (const chunk of stream) {
          const delta = chunk.choices[0]?.delta?.content;
          if (delta) {
            fullContent += delta;
            tokenCount++;
            controller.enqueue(encoder.encode(delta));
          }
        }

        const elapsed = Date.now() - startTime;
        console.log(`[LLM][${requestId}] âœ“ æ¨¡å‹è°ƒç”¨å®Œæˆ`);
        console.log(`[LLM][${requestId}]   è€—æ—¶: ${elapsed}ms`);
        console.log(`[LLM][${requestId}]   è¾“å‡º chunks: ${tokenCount}`);
        console.log(`[LLM][${requestId}]   å›å¤å†…å®¹: ${fullContent}`);

        // If no content was generated, send fallback
        if (!fullContent) {
          const fallback = getFallbackResponse(level);
          console.log(`[LLM][${requestId}]   âš  ç©ºå›å¤ï¼Œä½¿ç”¨ fallback`);
          controller.enqueue(encoder.encode(fallback));
        }

        controller.close();
      } catch (error) {
        const elapsed = Date.now() - startTime;
        console.error(`[LLM][${requestId}] âœ— æ¨¡å‹è°ƒç”¨å¤±è´¥ (${elapsed}ms):`, error);

        // Send fallback on error
        const fallback = getFallbackResponse(level);
        controller.enqueue(encoder.encode(fallback));
        controller.close();
      }
    },
  });
}

// ============================================================
// éæµå¼ç”Ÿæˆï¼ˆç”¨äºéœ€è¦å®Œæ•´æ–‡æœ¬çš„åœºæ™¯ï¼Œå¦‚å­˜åº“ï¼‰
// ============================================================
export async function generateCheckInResponse(
  level: string,
  question: string,
  todayCheckIns: { level: string; checkInAt: string }[],
  recentSummaries: { date: string; avgScore: number; minScore: number }[]
): Promise<string> {
  const context = buildCheckInContext(level, question, todayCheckIns, recentSummaries);
  const startTime = Date.now();
  const requestId = Math.random().toString(36).slice(2, 8);

  console.log(`[LLM][${requestId}] â–¶ æ¨¡å‹è°ƒç”¨å¼€å§‹ (éæµå¼)`);
  console.log(`[LLM][${requestId}]   æ¨¡å‹: ${LLM_MODEL}`);
  console.log(`[LLM][${requestId}]   ç²¾åŠ›ç­‰çº§: ${level}`);

  try {
    const response = await client.chat.completions.create({
      model: LLM_MODEL,
      messages: [
        { role: "system", content: COACH_SYSTEM_PROMPT },
        { role: "user", content: context },
      ],
      max_tokens: 200,
      temperature: 0.8,
    });

    const content = response.choices[0]?.message?.content || getFallbackResponse(level);
    const elapsed = Date.now() - startTime;
    const usage = response.usage;

    console.log(`[LLM][${requestId}] âœ“ æ¨¡å‹è°ƒç”¨å®Œæˆ`);
    console.log(`[LLM][${requestId}]   è€—æ—¶: ${elapsed}ms`);
    console.log(
      `[LLM][${requestId}]   tokens: prompt=${usage?.prompt_tokens}, completion=${usage?.completion_tokens}, total=${usage?.total_tokens}`
    );
    console.log(`[LLM][${requestId}]   å›å¤å†…å®¹: ${content}`);

    return content;
  } catch (error) {
    const elapsed = Date.now() - startTime;
    console.error(`[LLM][${requestId}] âœ— æ¨¡å‹è°ƒç”¨å¤±è´¥ (${elapsed}ms):`, error);
    return getFallbackResponse(level);
  }
}

function getFallbackResponse(level: string): string {
  const fallbacks: Record<string, string[]> = {
    HIGH: [
      "ä»Šå¤©çŠ¶æ€ä¸é”™å‘¢ï¼ŒèŠ±å›­é‡Œçš„èŠ±ä¹Ÿå¼€å¾—å¾ˆå¥½ ğŸŒ¸",
      "ç²¾åŠ›æ»¡æ»¡çš„ä¸€å¤©ï¼Œè®°å¾—ç•™ä¸€äº›ç»™è‡ªå·±ã€‚",
      "çŠ¶æ€å¾ˆå¥½ï¼ä¸è¿‡åˆ«å¿˜äº†ç»™èŠ±å›­ç•™ç‚¹æ°´ã€‚",
    ],
    MEDIUM: [
      "è¿˜ä¸é”™ï¼Œç¨³ç¨³çš„ã€‚è®°å¾—é€‚æ—¶ä¼‘æ¯ä¸€ä¸‹ã€‚",
      "ä¸­ç­‰æ°´ä½ï¼ŒèŠ±å›­è¿˜ç»¿ç€ã€‚æ³¨æ„åˆ«é€æ”¯å“¦ã€‚",
      "çŠ¶æ€å°šå¯ï¼Œç»§ç»­ä¿æŒèŠ‚å¥å°±å¥½ã€‚",
    ],
    LOW: [
      "è¾›è‹¦äº†ï¼ŒèŠ±å›­é‡Œçš„èŠ±æœ‰ç‚¹ç´¯äº†ã€‚æ‰¾ä¸ªæ—¶é—´æ­‡ä¸€æ­‡ï¼Ÿ",
      "ç²¾åŠ›æœ‰äº›ä½äº†ï¼Œè¦ä¸è¦æ”¾ä¸‹æ‰‹å¤´çš„äº‹ä¼‘æ¯ä¸€ä¼šå„¿ï¼Ÿ",
      "ä»Šå¤©æ¶ˆè€—ä¸å°‘å‘¢ã€‚æ˜¯æ—¶å€™å¯¹è‡ªå·±å¥½ä¸€ç‚¹äº†ã€‚",
    ],
    EXHAUSTED: [
      "ä½ çœŸçš„å¾ˆç´¯äº†ã€‚ç°åœ¨æœ€é‡è¦çš„äº‹æ˜¯ä¼‘æ¯ã€‚èŠ±å›­æ˜å¤©è¿˜åœ¨ã€‚",
      "è¯¥åœä¸‹æ¥äº†ã€‚æ²¡æœ‰ä»€ä¹ˆäº‹æ¯”ä½ è‡ªå·±æ›´é‡è¦ã€‚",
      "ä½ç”µé‡æ¨¡å¼äº†ã€‚æ”¾ä¸‹ä¸€åˆ‡ï¼Œå»ä¼‘æ¯å§ã€‚æ˜å¤©æ˜¯æ–°çš„ä¸€å¤©ã€‚",
    ],
  };

  const options = fallbacks[level] || fallbacks.MEDIUM;
  return options[Math.floor(Math.random() * options.length)];
}

export async function generateWeeklySummary(
  weeklySummaries: {
    date: string;
    avgScore: number;
    minScore: number;
    maxScore: number;
    belowReserve: number;
  }[],
  reserveRatio: number
): Promise<string> {
  const startTime = Date.now();
  const requestId = Math.random().toString(36).slice(2, 8);

  const context = `## æœ¬å‘¨ç²¾åŠ›æ•°æ®
${weeklySummaries
  .map(
    (s) =>
      `- ${s.date}ï¼ˆ${getDayOfWeek(s.date)}ï¼‰: å‡å€¼ ${s.avgScore.toFixed(2)}, æœ€ä½ ${s.minScore.toFixed(2)}, é€æ”¯ ${s.belowReserve} æ¬¡`
  )
  .join("\n")}

## ç”¨æˆ·è®¾å®š
- ç²¾åŠ›ä¿ç•™æ¯”ä¾‹: ${(reserveRatio * 100).toFixed(0)}%

## ä»»åŠ¡
è¯·ç”Ÿæˆä¸€ä»½æ¸©æš–çš„å‘¨åº¦ç²¾åŠ›å›é¡¾ï¼ŒåŒ…å«ï¼š
1. å¯¹è¿™å‘¨ç²¾åŠ›çŠ¶æ€çš„æ€»ç»“ï¼ˆ2-3å¥ï¼‰
2. å‘ç°çš„è§„å¾‹æˆ–å€¼å¾—æ³¨æ„çš„ç‚¹ï¼ˆ1-2å¥ï¼‰
3. å¯¹ä¸‹å‘¨çš„ä¸€ä¸ªå°å»ºè®®ï¼ˆ1å¥ï¼‰
æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ã€‚`;

  console.log(`[LLM][${requestId}] â–¶ å‘¨æŠ¥ç”Ÿæˆå¼€å§‹`);
  console.log(`[LLM][${requestId}]   æ¨¡å‹: ${LLM_MODEL}`);
  console.log(`[LLM][${requestId}]   æ•°æ®å¤©æ•°: ${weeklySummaries.length}`);

  try {
    const response = await client.chat.completions.create({
      model: LLM_MODEL,
      messages: [
        { role: "system", content: COACH_SYSTEM_PROMPT },
        { role: "user", content: context },
      ],
      max_tokens: 400,
      temperature: 0.7,
    });

    const content =
      response.choices[0]?.message?.content ||
      "æœ¬å‘¨ç²¾åŠ›æŠ¥å‘Šç”Ÿæˆä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚";
    const elapsed = Date.now() - startTime;
    const usage = response.usage;

    console.log(`[LLM][${requestId}] âœ“ å‘¨æŠ¥ç”Ÿæˆå®Œæˆ`);
    console.log(`[LLM][${requestId}]   è€—æ—¶: ${elapsed}ms`);
    console.log(
      `[LLM][${requestId}]   tokens: prompt=${usage?.prompt_tokens}, completion=${usage?.completion_tokens}, total=${usage?.total_tokens}`
    );
    console.log(`[LLM][${requestId}]   å›å¤å†…å®¹: ${content}`);

    return content;
  } catch (error) {
    const elapsed = Date.now() - startTime;
    console.error(`[LLM][${requestId}] âœ— å‘¨æŠ¥ç”Ÿæˆå¤±è´¥ (${elapsed}ms):`, error);
    return "æœ¬å‘¨ç²¾åŠ›æŠ¥å‘Šæš‚æ—¶æ— æ³•ç”Ÿæˆï¼Œè¯·ç¨åå†è¯•ã€‚";
  }
}

function getDayOfWeek(dateStr: string): string {
  const days = ["å‘¨æ—¥", "å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­"];
  return days[new Date(dateStr).getDay()];
}
